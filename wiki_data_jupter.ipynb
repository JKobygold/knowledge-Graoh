{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def get_sublinks(article_url):\n",
    "  # Make a request to the Wikipedia article\n",
    "  response = requests.get(article_url)\n",
    "\n",
    "  # Parse the HTML of the Wikipedia article\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "  # Find all the sublinks within the article\n",
    "  sublinks = []\n",
    "  for link in soup.find_all('a', href=True):\n",
    "    # Check if the link is a sublink (begins with '/wiki/')\n",
    "    if link['href'].startswith('/wiki/'):\n",
    "      sublinks.append(link['href'])\n",
    "\n",
    "  return sublinks  \n",
    "  \n",
    "sublinks = get_sublinks('https://en.wikipedia.org/wiki/Python_(programming_language)')\n",
    "\n",
    "article_url = 'https://en.wikipedia.org/wiki/Python_(programming_language)'\n",
    "\n",
    "# Get the name of the article from the URL\n",
    "article_name = re.search(r'wiki/(.+?)$', article_url).group(1)\n",
    "# Add a new column to the dataframe with the name of the article\n",
    "art_array = sublinks\n",
    "\n",
    "\n",
    "# Create a list of sublink dictionaries\n",
    "sublinks_data = []\n",
    "for sublink in sublinks:\n",
    "  sublink_dict = {'sublink': sublink}\n",
    "  sublinks_data.append(sublink_dict)\n",
    "\n",
    "# Create a Pandas dataframe from the list of dictionaries\n",
    "df = pd.DataFrame(sublinks_data)\n",
    "df.columns = [article_name]\n",
    "df['Python_(programming_language)'] = df['Python_(programming_language)'].apply(lambda x: x.replace('/wiki/', ''))\n",
    "df = df[~df['Python_(programming_language)'].str.contains(r':\\b')]\n",
    "df = df.drop_duplicates()\n",
    "df = df.reset_index(drop=True)\n",
    "df = df[~df['Python_(programming_language)'].str.contains(r'%{2}')]\n",
    "\n",
    "\n",
    "def is_valid_article(url):\n",
    "  # Make a request to the URL\n",
    "  try:\n",
    "    response = requests.get(url)\n",
    "  except:\n",
    "    return False\n",
    "\n",
    "  # Check if the response is a valid Wikipedia article\n",
    "  if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    if soup.find(id='firstHeading'):\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "# Add a new column to the dataframe indicating whether each sublink is a valid Wikipedia article\n",
    "df['is_valid_article'] = df['Python_(programming_language)'].apply(lambda x: is_valid_article(f'https://en.wikipedia.org/wiki/{x}'))\n",
    "df = df.drop(columns=['is_valid_article'])\n",
    "df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
